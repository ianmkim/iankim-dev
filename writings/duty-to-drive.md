The Duty to Drive
Why humans should be the only ones on the road
Ian M Kim
2021
0.4.2
---
# The Duty to Drive
Given the promise of safer-than-human levels of proficiency, mandating
self-driving cars seem like an attractive proposition at face value. The
following argument will prove that such a mandate would be a severe violation
of man’s right to an agentive life which outweighs its benefits. More
importantly, the very acceptance of unsupervised self-driving cars represents a
mass abdication of moral responsibility. This abdication will cheapen moral
agency and set a precedent for other technologies that our moral intuitions
reject. So much so that we should seriously consider a ban on self-driving
technology until a morally agentive AGI can be developed.

## Context & Data
First, let us put the problem in context: Human-driven cars are extraordinarily
safe. Since 2019, the U.S has recorded 36,096 auto deaths. That’s slightly over
one death per 100 million vehicle miles traveled. Out of these, over 16,783
deaths were caused by drivers with a blood alcohol concentration above 0.08%.
(IIHS) These statistics suggest that 1) fatal motor crashes are an extreme
rarity and 2) The human brain is more than capable of piloting a car in the
vast majority of the situations. To outright ban human-driven cars in the
context of these small numbers represents a dangerous infringement on some very
fundamental rights.

It seems uncontroversial to suggest that all men should be guaranteed an
agentive life so long as it does not infringe on others’ rights. We can easily
derive this from Nozick’s ‘Experience Machine’ thought experiment in which a
machine controls your body to an optimal life by bypassing your brain (Nozick,
264). Our moral intuitions reject such a system even if it produces an optimal
life because we are unsatisfied by being mere observers of life. Instead, we
want to actively participate in life and make decisions for ourselves (Ratoff,
15). Among the prerequisites for this right is mobility; since the means of
participating in civil society are physically separated from you, one cannot be
expected to exercise agency if he is relegated to his bed. One cannot vote,
purchase, love, enjoy, or do anything of substantive value in the absence of
mobility.

## Cars as a Necessity
At least in the U.S, cars are more than mere commodities; they are essential to
mobility. Aside from a small number of cities, it is difficult to say you have
full mobility without a car. Since mass auto manufacturing started in America
under specific economic and historical circumstances, cities were planned with
cars in mind (Buehler). Why plan a city where everything is within walking
distance when you can just drive 15 mins to get to a grocery store? Mass
adoption of cars meant that businesses critical to everyday life: banks,
grocery stores, etc., grew further apart. Due to this, in 2010, Americans drove
for 85% of their daily trips (Buehler). Particularly in suburban and rural
    areas, personal auto transport is essential. It is unreasonable to suggest
    that one still has “mobility” when he has to walk an hour to get to a
    grocery store, 90 minutes to the bank, and 40 minutes to the town hall.

An astute reader will point out that the government is not necessarily limiting
your mobility by mandating self-driving cars. After all, they are merely
mandating an alternative. But this is inconsistent with our moral intuitions
about government. Particularly for derivative rights/abilities/artifacts that
enable fundamental rights, we are more uncomfortable with the government
dictating how it’s supposed to be done in addition to whether it should be
done. Consider food: you currently have a wide variety of options for food.
Would you feel comfortable if the police mandated only the consumption of
government-curated food with just the right amount of nutrients and portions to
curb obesity and save the environment? Most moral intuitions would have
visceral reactions to such a policy even though it merely offers a materially
better alternative. We still find the government regulating the “how”
questionable, particularly if the object of regulation is fundamental. For an
activity with very low inherent risk (arguably lower than climate change and
obesity) and high importance to mobility and consequently an agentive life, a
government policy mandating self-driving cars seems morally unacceptable.

So sure, the government may not be allowed to mandate self-driving cars
legally, but are we individually morally required to switch to self-driving
cars? First, let us establish that driving consists of inherently ethical
choices because it is an inherently risky activity. As a driver, you are making
an ethical decision on distributing that risk among other cars, trucks,
pedestrians, cyclists, property, and wildlife (Goodall, 28). Consider for a
moment that you are on a three-lane road. To your right is a semi-truck, and to
your left is a small car; if you position the car left of center for your
safety, you’re distributing more risk to the smaller car, thereby making a
moral decision. (“Controlling Vehicle Lateral Lane Positioning” patent). Why
does that innocent car to your left deserve a higher risk of death? You can
begin to see how even the most routine actions in driving are moral decisions
when examined closer. This is not to suggest that all actions taken by drivers
are moral (as opposed to immoral) but they are moral in nature (as opposed to
amoral).

## Can AI be Moral Agents?
Inherently moral actions require inherently moral agents. We can easily derive
this from the statement that agents without moral agency cannot make moral
decisions by definition. But how can we determine moral agency? Allen et al.
introduce the “Moral Turing Test” (MTT) for this (Allen et al. 254). However,
unlike the original turing test, which rewards the deception of the judge, MTT
requires the judge to verify the ability of moral decision-making and its
justifications. Only then can we functionally accept that an agent is morally
agentive.

Willingly putting things incapable of moral reasoning in the position to make
moral actions is reprehensible. It would be wildly unethical for Justice
Gorsuch to relegate his opinion on abortion to a coin flip or the behaviors of
his five dogs (Kotta). Now sure, self-driving cars are far more complicated
than coinflips. Except, self-driving cars are only exactly that: just a more
complex coin flip.

Consider one of the most popular deep learning algorithms ResNET, derivations
of which are used in self-driving cars today and for the foreseeable future. It
is literally just a series of matrix multiplications that capture the
statistical significances of a given dataset (He et al. 3). There is nothing
miraculous about deep learning; its origins date back to 1958 with Rosenblat’s
“perceptron.” The essential structure of backpropagation and forward passes
remains primarily unchanged (Rosenblat, 386). ML now is essentially a slightly
more complicated version of standard statistics, which in and of itself is just
a more deterministic coin flip. The underlying mechanisms, no matter how much
we anthropomorphize them, are entirely arbitrary and stochastic (stochastic
gradient descent, stochastic initialization). Not only do these ML algorithms
fail the Moral Turing Test, they can’t even begin to take the test. Machine
learning algorithms are conclusively and unequivocally not moral agents.

But, can’t we hardcode ethics into self-driving cars by giving a justification
for possible situations it might encounter? This is an absurd proposition. It’s
    not even a matter of engineering difficulty; it is a matter of theoretical
    impossibility. Given that we can’t even hardcode traffic laws into cars due
    to unknown road conditions and given the sheer number of possible scenarios
    the car might encounter, hardcoding ethics into cars is impossible. Even
    the most practical formulation of such a comprehensive system requires
    impractical memory and computation time (Hutter, 21). Even so, hard coding
    ethics doesn’t solve the issue: imagine a tape recorder with infinite
    memory in which an ethicist records his decisions and justifications for
    every conceivable dilemma. You would hardly qualify that tape recorder as a
    moral agent. Unless we develop MTT-passing algorithms, we are left with
    stochastic and arbitrary matrices and calculus.

## Moral Responsibility
So when a human decides to let a fully autonomous car drive in his place, he is
essentially abdicating his moral responsibility to an object with no moral
agency. Some philosophers have invoked a “responsibility gap” to explain why
this is bad: how can we carry out justice if the algorithm is not a moral agent
we can punish (Nyholm)? However, men of sound mind and body have a prima facie
duty to not abdicate their moral agency. After all, the fundamental premise of
normative ethics is that men have prima facie duty to exercise their moral
agency. Therefore, moral abdication is a fundamental wrong that should be
squarely attributed to the individual who makes a rational decision to let an
algorithm drive.

But if matrix multiplications can drive a car better than a human, why not let
it? Suppose that Justice Gorsuch formulates an algebraic expression that
creates new decisions based on his previous decisions about as well as a
self-driving car can create new driving commands. Would you feel comfortable
with a piece of math making decisions for the court? You should be
uncomfortable with an emotionless, inert, intention-less math from making such
moral decisions. This proves that our intuitions care about the moral
decision-making process at least as much as the decision itself.

A certain consequentialist would argue that humans are bad moral reasoners
precisely because we are not emotionless and inert. This claim seems
paradoxical. To view perfect morality as one devoid of emotions is to reject
morality entirely. There is plenty of evidence to believe that empathy is a
precursor to morality: empathy innate in babies develops into moral agency in
adults. If we trace our genetic heritage, we find that our distant ancestor
species also possessed empathy, which later developed into human morality
(Granger, 11). It seems that from both individual and evolutionary
perspectives, emotion is a prerequisite for morality. After all, if we cannot
empathize with others, how can we make just and moral decisions? If
neuroscientists are to be believed, then perhaps self-driving cars should be
outlawed until we can produce emotions and subjective experiences on machines.
Then we would have no need for the MTT, simply because we can fundamentally
prove the possession of moral agency.

But I digress. Our willingness to accept mass abdication of moral
responsibility presents dangerous further implications. An automated system in
a COVID ward calculating what patients should get ventilators and who is left
to die would be permissible under our acceptance of self-driving cars. Or
automated targeting systems taking shots at combatants without human approval.
Perhaps there’s a difference in the weight of moral decisions these systems
possess, but these differences are only of degree, not kind. In all three
cases, the risk distributed to different humans by arbitrary algorithms is a
matter of life or death. One might argue that a warrior robot acts with
different intentions compared to the self-driving car when distributing risk.
This is an unwarranted anthropomorphization: neither algorithms (in all
likelihood, they will use the same algorithm) possess any intentions. In all
three cases, the user decided that the weight of these moral decisions was
beneath him, thereby abdicating his moral responsibilities. As the only species
capable of moral agency, we should safeguard moral decisions and not cheapen
its sanctity—both for its own sake and in fear of its consequences for other
technologies.

Therefore, we arrive at the fundamental immorality of level four and above
self-driving cars, no matter their ability to drive. Particularly if the driver
cannot intervene (i.e no steering wheels and pedals), he would be voluntarily
abdicating his moral responsibility by letting a non-moral agent make decisions
that require a moral agent. The argument also shows that even under the most
optimistic outlook for AI development, the creation of morally agentive AI
should not be assumed in philosophical debates because the difference between
morally agentive AI and ML is a radical difference in kind, not degree. If you
accept voluntary suicide as the only permissible circumstance for moral
abdication, then self-driving cars represent a grave denial of responsibility
in exchange for insignificant benefits.

## References
 * Allen, Colin, Gary Varner, and Jason Zinser. "Prolegomena to any future artificial moral agent." Journal of Experimental & Theoretical Artificial Intelligence 12, no. 3 (2000): 251-261.
 * Buehler, Ralph. “9 Reasons the U.S. Ended Up So Much More Car-Dependent Than Europe.” Bloomberg.com. Bloomberg, February 4, 2014. https://www.bloomberg.com/news/articles/2014-02-04/9-reasons-the-u-s-ended-up-so-much-more-car-dependent-than-europe.
 * Controlling vehicle lateral lane positioning. The United States. Patent Number: US8781670B2. Filed May 28, 2013. Issued July 15, 2014.
 * Goodall, Noah J. "Can you program ethics into a self-driving car?." IEEE Spectrum 53, no. 6 (2016): 28-58.
 * Granger, Richard. "Toward the quantification of cognition." arXiv preprint arXiv:2008.05580 (2020).
 * He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. "Deep residual learning for image recognition." In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770-778. 2016.
 * Hutter, Marcus. "A gentle introduction to the universal algorithmic agent AIXI." Artificial General Intelligence (2003).
 * Kotta, Kristine. “Neil Gorsuch's Wife, Louise: The Outdoorsy, Religious Brit Who Captured His Heart.” Fox News. FOX News Network, April 25, 2017. https://www.foxnews.com/politics/neil-gorsuchs-wife-louise-the-outdoorsy-religious-brit-who-captured-his-heart.
 * Nozick, Robert. "The experience machine." Ethical Theory: an Anthology (2013).
 * Nyholm, Sven. "The ethics of crashes with self‐driving cars: A roadmap, I." Philosophy Compass 13, no. 7 (2018): e12507.
 * Ratoff, W. MS. ‘Self-Driving Cars and the Right to Drive’
 * Rosenblat, F. "The perceptron: A probabilistic model for information storage and organization in the brain." Psychological Review 65, no. 6 (1958): 386-408.
 * “Fatality Facts 2019: State by State.” IIHS. Insurance Institute for Highway Safety. Accessed October 5, 2021. https://www.iihs.org/topics/fatality-statistics/detail/state-by-state.
